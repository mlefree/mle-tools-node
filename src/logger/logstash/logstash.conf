# mle-tools-node logstash v2.2
#   HOST_IP added + fix & improvments

input {
  file {
    path => "/app/logs/**/**.log"
    start_position => "beginning"
    codec => multiline {
      pattern => "^\d{4}-\d{2}-\d{2}T"
      negate => true
      what => "previous"
    }
    ignore_older => 0
    discover_interval => 2
    stat_interval => 1
    close_older => 3600
    max_open_files => 4096
    file_sort_by => "last_modified"
    file_sort_direction => "desc"
    check_archive_validity => false
    sincedb_write_interval => 15
  }
}
# should filter lines like :
# 2025-08-10T19:29:08.445Z|47316.0     |info : PERF - "Logger" started ###
# 2025-08-10T19:54:17.261Z|50043.0     |info : [Perf Monitoring] (PERF) - "Logger" started ###
filter {

    grok {
        match => {
            "message" => [
                "%{TIMESTAMP_ISO8601:timestamp}\|%{DATA:process_info}\|%{DATA:level}\s*:\s*\[%{DATA:service}\]\s+%{GREEDYDATA:log_message}",
                "%{TIMESTAMP_ISO8601:timestamp}\|%{DATA:process_info}\|%{DATA:level}\s*:\s+%{GREEDYDATA:log_message}"
            ]
        }
        break_on_match => true
        tag_on_failure => ["fail_match"]
    }

    # Extract thread information from process_info
    if [process_info] {
       mutate {
           gsub => [ "process_info", "^\s+|\s+$", "" ]  # Trim spaces
       }
       grok {
           match => { "process_info" => "^(?<process_id>\d+)\.(?<thread_id>\d+)$" }
           tag_on_failure => ["fail_process_info"]
       }
       if ![fail_process_info] {
           mutate {
               convert => { "process_id" => "integer" }
               convert => { "thread_id" => "integer" }
           }
       }
    }

    if [log_message] {
        grok {
          match => {
            "log_message" => "^\(%{DATA:tag}\)\s*%{GREEDYDATA:clean_message}"
          }
          tag_on_failure => []
        }

        if [tag] {
          mutate {
            replace => { "log_message" => "%{clean_message}" }
            add_tag => [ "%{tag}" ]
            remove_field => ["clean_message"]
          }
        }
    }

    date {
        match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSS" ]
        target => "@timestamp"
    }

    ruby {
        code => "
          begin
            time = event.get('@timestamp')
            if time
              event.set('year', time.year.to_s.rjust(4, '0'))
              event.set('month', time.month.to_s.rjust(2, '0'))
              event.set('day', time.day.to_s.rjust(2, '0'))
            else
              # Fallback to current date if timestamp parsing failed
              now = Time.now
              event.set('year', now.year.to_s.rjust(4, '0'))
              event.set('month', now.month.to_s.rjust(2, '0'))
              event.set('day', now.day.to_s.rjust(2, '0'))
            end
          rescue => e
            # Fallback on any error
            now = Time.now
            event.set('year', now.year.to_s.rjust(4, '0'))
            event.set('month', now.month.to_s.rjust(2, '0'))
            event.set('day', now.day.to_s.rjust(2, '0'))
          end
        "
    }

    if ![service] or [service] == "" {
        grok {
            match => { "[log][file][path]" => "/app/logs/%{WORD:service}/log-.*" }
            tag_on_failure => []
        }
    }

    mutate {
        add_field => { "[host][ip]" => "${HOST_IP:unknown}" }
    }

    # Cleanup
    mutate {
        remove_field => ["year_temp", "month_temp", "day_temp", "time", "tag"]
    }
    if "multiline" in [tags] {
      mutate {
        remove_tag => ["multiline"]
      }
    }

}

output {
  stdout { codec => rubydebug }
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "app-logs"
    manage_template => false
    data_stream => false
  }
}
